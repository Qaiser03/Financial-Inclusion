# Pipeline Configuration Parameters
# All paths are relative to repository root

# Random seeds for reproducibility
seeds:
  figure_generation: 42
  sampling: 42
  clustering: 42
  network_layout: 42

# Database preferences
deduplication:
  # When metadata completeness scores are equal and both records have references,
  # prefer this database for tie-breaking
  preferred_db: "scopus"  # Options: "scopus" or "wos"
  
  # Minimum metadata completeness score to keep a record (0-100)
  min_completeness_score: 0

# File paths
paths:
  raw_data:
    scopus: "data/raw/scopus_fi.csv"
    wos: "data/raw/wos_fi.txt"
  
  processed_data:
    scopus_cleaned: "data/processed/scopus_cleaned.csv"
    wos_cleaned: "data/processed/wos_cleaned.csv"
    canonical: "data/processed/canonical_fi.csv"
  
  fit_dictionary: "docs/FIT_DICTIONARY.yml"
  thesaurus: "vosviewer/thesaurus_fi.txt"
  
  outputs:
    audits: "outputs/audits"
    figures: "outputs/figures"
    tables: "outputs/tables"
  
  vosviewer:
    exports: "vosviewer/exports"
    cocitation_network: "vosviewer/exports/cocitation_network"

# Figure generation settings
figures:
  # Output formats: ["png", "pdf"] or ["png"] or ["pdf"]
  formats: ["png", "pdf"]
  
  # DPI for PNG exports
  dpi: 300
  
  # Figure sizes (width, height in inches)
  sizes:
    standard: [10, 6]
    wide: [12, 6]
    square: [8, 8]
    tall: [8, 10]
  
  # Wordcloud settings
  wordcloud:
    max_words: 100
    width: 800
    height: 400
    background_color: "white"
  
  # Network visualization settings
  network:
    node_size_range: [50, 500]
    edge_width_range: [0.5, 3.0]
    layout_algorithm: "spring"  # Options: "spring", "kamada_kawai", "circular"

# Table generation settings
tables:
  # Number of decimal places for numeric columns
  decimal_places: 2
  
  # CSV export settings
  csv:
    index: false
    encoding: "utf-8-sig"  # UTF-8 with BOM for Excel compatibility

# FIT tagging settings
fit_tagging:
  # Minimum term match confidence (if applicable)
  min_confidence: 0.0
  
  # Case sensitivity for term matching
  case_sensitive: false
  
  # Match whole words only (vs. substring matching)
  whole_words_only: true

# Data processing settings
processing:
  # Maximum number of records to process (for testing; set to null for all)
  max_records: null
  
  # Chunk size for large file processing
  chunk_size: 10000
  
  # Enable verbose logging
  verbose: true

# VOSviewer integration
vosviewer:
  # VOSviewer version used (document for reproducibility)
  version: "latest"
  
  # Network construction parameters (document manual settings)
  network_params:
    # These are documented but set manually in VOSviewer
    # Minimum number of citations for inclusion
    min_citations: 1
    # Normalization method
    normalization: "association_strength"

# ============================================================
# ADVANCED BIBLIOMETRIC ANALYSIS (v2.0.0)
# ============================================================

# Topic Modeling (LDA) settings
topic_modeling:
  # Number of topics to extract
  n_topics: 8
  
  # Random seed for LDA reproducibility
  seed: 42
  
  # Number of iterations for model training
  n_iterations: 500
  
  # Number of top terms to display per topic
  top_n_terms: 10
  
  # Minimum document frequency for term inclusion
  min_df: 5
  
  # Maximum document frequency ratio for term exclusion
  max_df: 0.8
  
  # Backend preference: "gensim" or "sklearn"
  # Falls back to sklearn if gensim unavailable
  backend: "gensim"
  
  # Text field to analyze (must exist in canonical data)
  text_field: "title"
  
  # Stop words language
  stop_words: "english"

# Citation Burst Detection settings
citation_bursts:
  # Z-score threshold for surge detection
  z_threshold: 2.0
  
  # Detection method: "full_series" or "rolling"
  method: "full_series"
  
  # Window size for rolling method (years)
  rolling_window: 5
  
  # Minimum citations per year to consider
  min_yearly_citations: 10
  
  # Years field in data
  year_field: "year_clean"
  
  # Citations field in data
  citation_field: "cited_by"

# Network Analysis settings
network_analysis:
  # Co-authorship network
  coauthorship:
    # Minimum collaborations to include an edge
    min_edge_weight: 1
    
    # Maximum number of authors to include (by degree centrality)
    top_n_authors: 50
    
    # Author field delimiter (for splitting author strings)
    author_delimiter: "; "
  
  # Keyword Co-occurrence network
  keyword_cooccurrence:
    # Minimum co-occurrences to include an edge
    min_edge_weight: 2
    
    # Maximum number of keywords to include
    top_n_keywords: 50
    
    # Keyword field in data
    keyword_field: "keywords_combined"
    
    # Keyword delimiter
    keyword_delimiter: "; "
  
  # Network visualization
  visualization:
    # Layout algorithm: "spring", "kamada_kawai", "circular"
    layout: "spring"
    
    # Spring layout parameters
    spring_k: null  # null for auto, or float
    spring_iterations: 50
    
    # Node sizing method: "degree", "betweenness", "uniform"
    node_size_method: "degree"
    
    # Node size range [min, max]
    node_size_range: [100, 1000]
    
    # Edge width range [min, max]
    edge_width_range: [0.5, 3.0]
    
    # Show top N labels only (to avoid clutter)
    top_n_labels: 20
    
    # Random seed for layout
    seed: 42

# Metrics computation settings
metrics:
  # Fields for entity extraction
  author_field: "authors"
  country_field: "affiliations"  # Will be parsed for country info
  institution_field: "affiliations"
  source_field: "source_title"
  
  # Citation field
  citation_field: "cited_by"
  
  # Top N entities to report
  top_n: 20

